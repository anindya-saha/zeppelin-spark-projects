{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Window Functions in PySPark\n",
    "\n",
    "Window Functions helps us to compare current row with other rows in the same dataframe, calculating running totals , sequencing of events and sessionization of transactions etc. This notebook is compiled from the following blog posts:\n",
    "\n",
    "+ https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html \n",
    "+ http://alvinhenrick.com/2017/05/16/apache-spark-analytical-window-functions/ \n",
    "+ https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-functions-windows.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting random seed for notebook reproducability\n",
    "rnd_seed=23\n",
    "np.random.seed=23\n",
    "np.random.set_state=23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/anindyas/work/spark-2.2.0-bin-hadoop2.6'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['SPARK_HOME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\").appName(\"retail_database_analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.55.42.56:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>retail_database_analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x118112908>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.55.42.56:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>retail_database_analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=retail_database_analysis>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x11811a588>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext = SQLContext(spark.sparkContext)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Employee Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Sample Dataframe\n",
    "employees = [\n",
    "    (7369, \"SMITH\", \"CLERK\", 7902, \"17-Dec-80\", 800, 20, 10),\n",
    "    (7499, \"ALLEN\", \"SALESMAN\", 7698, \"20-Feb-81\", 1600, 300, 30),\n",
    "    (7521, \"WARD\", \"SALESMAN\", 7698, \"22-Feb-81\", 1250, 500, 30),\n",
    "    (7566, \"JONES\", \"MANAGER\", 7839, \"2-Apr-81\", 2975, 0, 20),\n",
    "    (7654, \"MARTIN\", \"SALESMAN\", 7698, \"28-Sep-81\", 1250, 1400, 30),\n",
    "    (7698, \"BLAKE\", \"MANAGER\", 7839, \"1-May-81\", 2850, 0, 30),\n",
    "    (7782, \"CLARK\", \"MANAGER\", 7839, \"9-Jun-81\", 2450, 0, 10),\n",
    "    (7788, \"SCOTT\", \"ANALYST\", 7566, \"19-Apr-87\", 3000, 0, 20),\n",
    "    (7629, \"ALEX\", \"SALESMAN\", 7698, \"28-Sep-79\", 1150, 1400, 30),\n",
    "    (7839, \"KING\", \"PRESIDENT\", 0, \"17-Nov-81\", 5000, 0, 10),\n",
    "    (7844, \"TURNER\", \"SALESMAN\", 7698, \"8-Sep-81\", 1500, 0, 30),\n",
    "    (7876, \"ADAMS\", \"CLERK\", 7788, \"23-May-87\", 1100, 0, 20)    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emp_df = spark.createDataFrame(employees, [\"empno\", \"ename\", \"job\", \"mgr\", \"hiredate\", \"sal\", \"comm\", \"deptno\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+---------+----+----+------+\n",
      "|empno| ename|      job| mgr| hiredate| sal|comm|deptno|\n",
      "+-----+------+---------+----+---------+----+----+------+\n",
      "| 7369| SMITH|    CLERK|7902|17-Dec-80| 800|  20|    10|\n",
      "| 7499| ALLEN| SALESMAN|7698|20-Feb-81|1600| 300|    30|\n",
      "| 7521|  WARD| SALESMAN|7698|22-Feb-81|1250| 500|    30|\n",
      "| 7566| JONES|  MANAGER|7839| 2-Apr-81|2975|   0|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698|28-Sep-81|1250|1400|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839| 1-May-81|2850|   0|    30|\n",
      "| 7782| CLARK|  MANAGER|7839| 9-Jun-81|2450|   0|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566|19-Apr-87|3000|   0|    20|\n",
      "| 7629|  ALEX| SALESMAN|7698|28-Sep-79|1150|1400|    30|\n",
      "| 7839|  KING|PRESIDENT|   0|17-Nov-81|5000|   0|    10|\n",
      "| 7844|TURNER| SALESMAN|7698| 8-Sep-81|1500|   0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788|23-May-87|1100|   0|    20|\n",
      "+-----+------+---------+----+---------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: long (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: long (nullable = true)\n",
      " |-- hiredate: string (nullable = true)\n",
      " |-- sal: long (nullable = true)\n",
      " |-- comm: long (nullable = true)\n",
      " |-- deptno: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emp_df.createOrReplaceTempView(\"emp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def strip_margin(text):\n",
    "    nomargin = re.sub('\\n[ \\t]*\\|', ' ', text)\n",
    "    trimmed = re.sub('\\s+', ' ', nomargin)\n",
    "    return trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank salary within each department:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL Using rank() Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+----+\n",
      "|empno| ename|      job|deptno| sal|rank|\n",
      "+-----+------+---------+------+----+----+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|   1|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|   2|\n",
      "| 7369| SMITH|    CLERK|    10| 800|   3|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|   1|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|   2|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|   3|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|   4|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|   4|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|   6|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|   1|\n",
      "| 7566| JONES|  MANAGER|    20|2975|   2|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|   3|\n",
      "+-----+------+---------+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, RANK() OVER (partition by deptno ORDER BY sal desc) as rank\n",
    "          |FROM emp\n",
    "        \"\"\")).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using Window Function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we will need to define the window we will be working on i.e. we will partition by department (deptno) and order by salary (sal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+----+\n",
      "|empno| ename|      job|deptno| sal|rank|\n",
      "+-----+------+---------+------+----+----+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|   1|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|   2|\n",
      "| 7369| SMITH|    CLERK|    10| 800|   3|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|   1|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|   2|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|   3|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|   4|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|   4|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|   6|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|   1|\n",
      "| 7566| JONES|  MANAGER|    20|2975|   2|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|   3|\n",
      "+-----+------+---------+------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.rank().over(windowSpec).alias('rank')).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Rank salary within each department:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL Using dense_rank() Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+----------+\n",
      "|empno| ename|      job|deptno| sal|dense_rank|\n",
      "+-----+------+---------+------+----+----------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|         1|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|         2|\n",
      "| 7369| SMITH|    CLERK|    10| 800|         3|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|         1|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|         2|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|         3|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|         4|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|         4|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|         5|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|         1|\n",
      "| 7566| JONES|  MANAGER|    20|2975|         2|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|         3|\n",
      "+-----+------+---------+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, DENSE_RANK() OVER (partition by deptno ORDER BY sal desc) as dense_rank\n",
    "          |FROM emp\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+----------+\n",
      "|empno| ename|      job|deptno| sal|dense_rank|\n",
      "+-----+------+---------+------+----+----------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|         1|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|         2|\n",
      "| 7369| SMITH|    CLERK|    10| 800|         3|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|         1|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|         2|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|         3|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|         4|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|         4|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|         5|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|         1|\n",
      "| 7566| JONES|  MANAGER|    20|2975|         2|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|         3|\n",
      "+-----+------+---------+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.dense_rank().over(windowSpec).alias('dense_rank')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row Number within each department:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL Using row_num() Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+-------+\n",
      "|empno| ename|      job|deptno| sal|row_num|\n",
      "+-----+------+---------+------+----+-------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|      1|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|      2|\n",
      "| 7369| SMITH|    CLERK|    10| 800|      3|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|      1|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|      2|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|      3|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|      4|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|      5|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|      6|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|      1|\n",
      "| 7566| JONES|  MANAGER|    20|2975|      2|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|      3|\n",
      "+-----+------+---------+------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, ROW_NUMBER() OVER (partition by deptno ORDER BY sal desc) as row_num\n",
    "          |FROM emp\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+-------+\n",
      "|empno| ename|      job|deptno| sal|row_num|\n",
      "+-----+------+---------+------+----+-------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|      1|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|      2|\n",
      "| 7369| SMITH|    CLERK|    10| 800|      3|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|      1|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|      2|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|      3|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|      4|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|      5|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|      6|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|      1|\n",
      "| 7566| JONES|  MANAGER|    20|2975|      2|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|      3|\n",
      "+-----+------+---------+------+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.row_number().over(windowSpec).alias('row_num')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Total (Salary) within each department:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL Using sum() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+-------------+\n",
      "|empno| ename|      job|deptno| sal|running_total|\n",
      "+-----+------+---------+------+----+-------------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|         5000|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|         7450|\n",
      "| 7369| SMITH|    CLERK|    10| 800|         8250|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|         2850|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|         4450|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|         5950|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|         8450|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|         8450|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|         9600|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|         3000|\n",
      "| 7566| JONES|  MANAGER|    20|2975|         5975|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|         7075|\n",
      "+-----+------+---------+------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, SUM(sal) OVER (partition by deptno ORDER BY sal desc) as running_total\n",
    "          |FROM emp\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using sum() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+-------------+\n",
      "|empno| ename|      job|deptno| sal|running_total|\n",
      "+-----+------+---------+------+----+-------------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|         5000|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|         7450|\n",
      "| 7369| SMITH|    CLERK|    10| 800|         8250|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|         2850|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|         4450|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|         5950|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|         8450|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|         8450|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|         9600|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|         3000|\n",
      "| 7566| JONES|  MANAGER|    20|2975|         5975|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|         7075|\n",
      "+-----+------+---------+------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.sum('sal').over(windowSpec).alias('running_total')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc()).rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+----------+\n",
      "|empno| ename|      job|deptno| sal|moving_avg|\n",
      "+-----+------+---------+------+----+----------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|    5000.0|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|    3725.0|\n",
      "| 7369| SMITH|    CLERK|    10| 800|    2750.0|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|    2850.0|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|    2225.0|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|   1983.33|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|    1800.0|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|    1690.0|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|    1600.0|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|    3000.0|\n",
      "| 7566| JONES|  MANAGER|    20|2975|    2987.5|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|   2358.33|\n",
      "+-----+------+---------+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.round(F.avg('sal').over(windowSpec), 2).alias('moving_avg')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Salary within each department:\n",
    "Lead function allows us to compare current row with subsequent rows within each partition depending on the second argument (offset) which is by default set to 1 i.e. next row but we can change that parameter 2 to compare against every other row. The 3rd parameter is default value to be returned when no subsequent values exists or null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL Using lead() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+--------+\n",
      "|empno| ename|      job|deptno| sal|next_val|\n",
      "+-----+------+---------+------+----+--------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|    2450|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|     800|\n",
      "| 7369| SMITH|    CLERK|    10| 800|    null|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|    1600|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|    1500|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|    1250|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|    1250|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|    1150|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|    null|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|    2975|\n",
      "| 7566| JONES|  MANAGER|    20|2975|    1100|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|    null|\n",
      "+-----+------+---------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, LEAD(sal, 1) OVER (partition by deptno ORDER BY sal desc) as next_val\n",
    "          |FROM emp\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using lead() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+--------+\n",
      "|empno| ename|      job|deptno| sal|next_val|\n",
      "+-----+------+---------+------+----+--------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|    2450|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|     800|\n",
      "| 7369| SMITH|    CLERK|    10| 800|       0|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|    1600|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|    1500|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|    1250|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|    1250|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|    1150|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|       0|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|    2975|\n",
      "| 7566| JONES|  MANAGER|    20|2975|    1100|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|       0|\n",
      "+-----+------+---------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.lead('sal', count=1, default=0).over(windowSpec).alias('next_val')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous Salary within each department:\n",
    "Lag function allows us to compare current row with preceding rows within each partition depending on the second argument (offset) which is by default set to 1 i.e. next row but we can change that parameter 2 to compare against every other row. The 3rd parameter is default value to be returned when no subsequent values exists or null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL Using lag() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+--------+\n",
      "|empno| ename|      job|deptno| sal|prev_val|\n",
      "+-----+------+---------+------+----+--------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|    null|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|    5000|\n",
      "| 7369| SMITH|    CLERK|    10| 800|    2450|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|    null|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|    2850|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|    1600|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|    1500|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|    1250|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|    1250|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|    null|\n",
      "| 7566| JONES|  MANAGER|    20|2975|    3000|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|    2975|\n",
      "+-----+------+---------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, LAG(sal, 1) OVER (partition by deptno ORDER BY sal desc) as prev_val\n",
    "          |FROM emp\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using lag() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+--------+\n",
      "|empno| ename|      job|deptno| sal|prev_val|\n",
      "+-----+------+---------+------+----+--------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|       0|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|    5000|\n",
      "| 7369| SMITH|    CLERK|    10| 800|    2450|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|       0|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|    2850|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|    1600|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|    1500|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|    1250|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|    1250|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|       0|\n",
      "| 7566| JONES|  MANAGER|    20|2975|    3000|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|    2975|\n",
      "+-----+------+---------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.lag('sal', count=1, default=0).over(windowSpec).alias('prev_val')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Salary within each department:\n",
    "First value within each partition .i.e. highest salary (we are using order by descending) within each department can be compared against every member within each department."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL Using first() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+---------+\n",
      "|empno| ename|      job|deptno| sal|first_val|\n",
      "+-----+------+---------+------+----+---------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|     5000|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|     5000|\n",
      "| 7369| SMITH|    CLERK|    10| 800|     5000|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|     2850|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|     2850|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|     2850|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|     2850|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|     2850|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|     2850|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|     3000|\n",
      "| 7566| JONES|  MANAGER|    20|2975|     3000|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|     3000|\n",
      "+-----+------+---------+------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, FIRST_VALUE(sal) OVER (partition by deptno ORDER BY sal desc) as first_val\n",
    "          |FROM emp\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using first() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+---------+\n",
      "|empno| ename|      job|deptno| sal|first_val|\n",
      "+-----+------+---------+------+----+---------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|     5000|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|     5000|\n",
      "| 7369| SMITH|    CLERK|    10| 800|     5000|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|     2850|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|     2850|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|     2850|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|     2850|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|     2850|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|     2850|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|     3000|\n",
      "| 7566| JONES|  MANAGER|    20|2975|     3000|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|     3000|\n",
      "+-----+------+---------+------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.first('sal').over(windowSpec).alias('first_val')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last Salary within each department:\n",
    "Last value within each partition .i.e. lowest salary (we are using order by descending) within each department can be compared against every member within each department."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL Using last() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+--------+\n",
      "|empno| ename|      job|deptno| sal|last_val|\n",
      "+-----+------+---------+------+----+--------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|    5000|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|    2450|\n",
      "| 7369| SMITH|    CLERK|    10| 800|     800|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|    2850|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|    1600|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|    1500|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|    1250|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|    1250|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|    1150|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|    3000|\n",
      "| 7566| JONES|  MANAGER|    20|2975|    2975|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|    1100|\n",
      "+-----+------+---------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, LAST_VALUE(sal) OVER (partition by deptno ORDER BY sal desc) as last_val\n",
    "          |FROM emp\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oops!** what happened here the `last_val` has the same value as in `sal` column but we were expecting the lowest salary within the department in the last_val column so for that we really need to understand how the window operates and works. There are two types of frames `ROW` and `RANGE`.The details are explained in this [posts](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html) from databricks.\n",
    "\n",
    "*This happens because **default window frame is range between unbounded preceding and current row**, so the `last_value()` never looks beyond current row unless we change the frame.*\n",
    "\n",
    "Last value fixed by supplying the window frame for `last_val()` to operate on. We will be using start frame current row and end frame unbounded following to get the last value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+--------+\n",
      "|empno| ename|      job|deptno| sal|last_val|\n",
      "+-----+------+---------+------+----+--------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|     800|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|     800|\n",
      "| 7369| SMITH|    CLERK|    10| 800|     800|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|    1150|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|    1150|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|    1150|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|    1150|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|    1150|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|    1150|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|    1100|\n",
      "| 7566| JONES|  MANAGER|    20|2975|    1100|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|    1100|\n",
      "+-----+------+---------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT empno, ename, job, deptno, sal, \n",
    "          |    LAST_VALUE(sal) OVER (partition by deptno ORDER BY sal desc ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) as last_val\n",
    "          |FROM emp\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using last() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('deptno')).orderBy(col('sal').desc()).rowsBetween(Window.currentRow, Window.unboundedFollowing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----+--------+\n",
      "|empno| ename|      job|deptno| sal|last_val|\n",
      "+-----+------+---------+------+----+--------+\n",
      "| 7839|  KING|PRESIDENT|    10|5000|     800|\n",
      "| 7782| CLARK|  MANAGER|    10|2450|     800|\n",
      "| 7369| SMITH|    CLERK|    10| 800|     800|\n",
      "| 7698| BLAKE|  MANAGER|    30|2850|    1150|\n",
      "| 7499| ALLEN| SALESMAN|    30|1600|    1150|\n",
      "| 7844|TURNER| SALESMAN|    30|1500|    1150|\n",
      "| 7521|  WARD| SALESMAN|    30|1250|    1150|\n",
      "| 7654|MARTIN| SALESMAN|    30|1250|    1150|\n",
      "| 7629|  ALEX| SALESMAN|    30|1150|    1150|\n",
      "| 7788| SCOTT|  ANALYST|    20|3000|    1100|\n",
      "| 7566| JONES|  MANAGER|    20|2975|    1100|\n",
      "| 7876| ADAMS|    CLERK|    20|1100|    1100|\n",
      "+-----+------+---------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select('empno', 'ename', 'job', 'deptno', 'sal', F.last('sal').over(windowSpec).alias('last_val')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Product Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product_revenues = [\n",
    "  (\"Thin\",       \"cell phone\", 6000),\n",
    "  (\"Normal\",     \"tablet\",     1500),\n",
    "  (\"Mini\",       \"tablet\",     5500),\n",
    "  (\"Ultra thin\", \"cell phone\", 5000),\n",
    "  (\"Very thin\",  \"cell phone\", 6000),\n",
    "  (\"Big\",        \"tablet\",     2500),\n",
    "  (\"Bendable\",   \"cell phone\", 3000),\n",
    "  (\"Foldable\",   \"cell phone\", 3000),\n",
    "  (\"Pro\",        \"tablet\",     4500),\n",
    "  (\"Pro2\",       \"tablet\",     6500)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prod_rev_df = spark.createDataFrame(product_revenues, [\"product\", \"category\", \"revenue\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+\n",
      "|   product|  category|revenue|\n",
      "+----------+----------+-------+\n",
      "|      Thin|cell phone|   6000|\n",
      "|    Normal|    tablet|   1500|\n",
      "|      Mini|    tablet|   5500|\n",
      "|Ultra thin|cell phone|   5000|\n",
      "| Very thin|cell phone|   6000|\n",
      "|       Big|    tablet|   2500|\n",
      "|  Bendable|cell phone|   3000|\n",
      "|  Foldable|cell phone|   3000|\n",
      "|       Pro|    tablet|   4500|\n",
      "|      Pro2|    tablet|   6500|\n",
      "+----------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prod_rev_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- revenue: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prod_rev_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prod_rev_df.createOrReplaceTempView(\"prod_rev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the best-selling and the second best-selling products in every category?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL: Using dense_rank() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+----------+\n",
      "|   product|  category|revenue|dense_rank|\n",
      "+----------+----------+-------+----------+\n",
      "|      Pro2|    tablet|   6500|         1|\n",
      "|      Mini|    tablet|   5500|         2|\n",
      "|      Thin|cell phone|   6000|         1|\n",
      "| Very thin|cell phone|   6000|         1|\n",
      "|Ultra thin|cell phone|   5000|         2|\n",
      "+----------+----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT product, category, revenue, dense_rank\n",
    "          |FROM (\n",
    "          |    SELECT product, category, revenue, \n",
    "          |    DENSE_RANK(revenue) OVER (partition by category ORDER BY revenue desc) as dense_rank\n",
    "          |    FROM prod_rev ) a\n",
    "          |WHERE dense_rank <= 2\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API Using dense_rank() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.partitionBy(col('category')).orderBy(col('revenue').desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+----------+\n",
      "|   product|  category|revenue|dense_rank|\n",
      "+----------+----------+-------+----------+\n",
      "|      Pro2|    tablet|   6500|         1|\n",
      "|      Mini|    tablet|   5500|         2|\n",
      "|      Thin|cell phone|   6000|         1|\n",
      "| Very thin|cell phone|   6000|         1|\n",
      "|Ultra thin|cell phone|   5000|         2|\n",
      "+----------+----------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(prod_rev_df\n",
    " .select('product', 'category', 'revenue', F.dense_rank().over(windowSpec).alias('dense_rank'))\n",
    " .where('dense_rank <= 2')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the difference between the revenue of each product and the revenue of the best selling product in the same category as that product?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL: Using RANGE BETWEEN with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+-------+--------+\n",
      "|   product|  category|revenue|max_rev|rev_diff|\n",
      "+----------+----------+-------+-------+--------+\n",
      "|      Pro2|    tablet|   6500|   6500|       0|\n",
      "|      Mini|    tablet|   5500|   6500|    1000|\n",
      "|       Pro|    tablet|   4500|   6500|    2000|\n",
      "|       Big|    tablet|   2500|   6500|    4000|\n",
      "|    Normal|    tablet|   1500|   6500|    5000|\n",
      "|      Thin|cell phone|   6000|   6000|       0|\n",
      "| Very thin|cell phone|   6000|   6000|       0|\n",
      "|Ultra thin|cell phone|   5000|   6000|    1000|\n",
      "|  Bendable|cell phone|   3000|   6000|    3000|\n",
      "|  Foldable|cell phone|   3000|   6000|    3000|\n",
      "+----------+----------+-------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT product, category, revenue, \n",
    "          |    MAX(revenue) OVER (partition by category ORDER BY revenue desc RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as max_rev,\n",
    "          |    MAX(revenue) OVER (partition by category ORDER BY revenue desc RANGE BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) - revenue as rev_diff \n",
    "          |FROM prod_rev\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API: Using rangeBetween() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = (Window\n",
    "              .partitionBy(prod_rev_df['category'])\n",
    "              .orderBy(prod_rev_df['revenue'].desc())\n",
    "              .rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------+-------+--------+\n",
      "|   product|  category|revenue|max_rev|rev_diff|\n",
      "+----------+----------+-------+-------+--------+\n",
      "|      Pro2|    tablet|   6500|   6500|       0|\n",
      "|      Mini|    tablet|   5500|   6500|    1000|\n",
      "|       Pro|    tablet|   4500|   6500|    2000|\n",
      "|       Big|    tablet|   2500|   6500|    4000|\n",
      "|    Normal|    tablet|   1500|   6500|    5000|\n",
      "|      Thin|cell phone|   6000|   6000|       0|\n",
      "| Very thin|cell phone|   6000|   6000|       0|\n",
      "|Ultra thin|cell phone|   5000|   6000|    1000|\n",
      "|  Bendable|cell phone|   3000|   6000|    3000|\n",
      "|  Foldable|cell phone|   3000|   6000|    3000|\n",
      "+----------+----------+-------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(prod_rev_df\n",
    " .select('product', 'category', 'revenue', \n",
    "             F.max('revenue').over(windowSpec).alias('max_rev'), \n",
    "             (F.max('revenue').over(windowSpec) - col('revenue')).alias('rev_diff'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Expense Data\n",
    "\n",
    "In this example dataset, there are two customers who have spent different amounts of money each day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expense = [\n",
    "    ('Alice', '5/1/2016', 50),\n",
    "    ('Alice', '5/2/2016', 45),\n",
    "    ('Alice', '5/3/2016', 55),\n",
    "    ('Alice', '5/4/2016', 47),\n",
    "    ('Alice', '5/5/2016', 19),\n",
    "    ('Bob', '5/1/2016', 25),\n",
    "    ('Bob', '5/4/2016', 29),\n",
    "    ('Bob', '5/9/2016', 27),\n",
    "    ('Bob', '5/12/2016', 32),\n",
    "    ('Bob', '5/13/2016', 10)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expense_df = spark.createDataFrame(expense, [\"name\", \"date\", \"amount_spent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------------+\n",
      "| name|     date|amount_spent|\n",
      "+-----+---------+------------+\n",
      "|Alice| 5/1/2016|          50|\n",
      "|Alice| 5/2/2016|          45|\n",
      "|Alice| 5/3/2016|          55|\n",
      "|Alice| 5/4/2016|          47|\n",
      "|Alice| 5/5/2016|          19|\n",
      "|  Bob| 5/1/2016|          25|\n",
      "|  Bob| 5/4/2016|          29|\n",
      "|  Bob| 5/9/2016|          27|\n",
      "|  Bob|5/12/2016|          32|\n",
      "|  Bob|5/13/2016|          10|\n",
      "+-----+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expense_df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expense_df.createOrReplaceTempView(\"expense\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the 3 days moving average for each customer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL: Using ROWS BETWEEN with specific values with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------------+----------+\n",
      "| name|     date|amount_spent|moving_avg|\n",
      "+-----+---------+------------+----------+\n",
      "|Alice| 5/3/2016|          55|      49.0|\n",
      "|Alice| 5/1/2016|          50|      47.5|\n",
      "|Alice| 5/4/2016|          47|     40.33|\n",
      "|Alice| 5/2/2016|          45|      50.0|\n",
      "|Alice| 5/5/2016|          19|      33.0|\n",
      "|  Bob| 5/1/2016|          25|      28.5|\n",
      "|  Bob|5/13/2016|          10|     23.67|\n",
      "|  Bob|5/12/2016|          32|     22.33|\n",
      "|  Bob| 5/4/2016|          29|      22.0|\n",
      "|  Bob| 5/9/2016|          27|      28.0|\n",
      "+-----+---------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT name, date, amount_spent, \n",
    "          |    ROUND(AVG(amount_spent) OVER (partition by name ORDER BY date ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING), 2) as moving_avg\n",
    "          |FROM expense\n",
    "          |ORDER BY name\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API: Using ROWS BETWEEN with specific values with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = (Window\n",
    "              .partitionBy(expense_df['name'])\n",
    "              .orderBy(expense_df['date'])\n",
    "              .rowsBetween(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------------+----------+\n",
      "| name|     date|amount_spent|moving_avg|\n",
      "+-----+---------+------------+----------+\n",
      "|Alice| 5/1/2016|          50|      47.5|\n",
      "|Alice| 5/2/2016|          45|      50.0|\n",
      "|Alice| 5/3/2016|          55|      49.0|\n",
      "|Alice| 5/4/2016|          47|     40.33|\n",
      "|Alice| 5/5/2016|          19|      33.0|\n",
      "|  Bob| 5/1/2016|          25|      28.5|\n",
      "|  Bob|5/12/2016|          32|     22.33|\n",
      "|  Bob|5/13/2016|          10|     23.67|\n",
      "|  Bob| 5/4/2016|          29|      22.0|\n",
      "|  Bob| 5/9/2016|          27|      28.0|\n",
      "+-----+---------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(expense_df\n",
    " .select('name', 'date', 'amount_spent', F.round(F.avg('amount_spent').over(windowSpec), 2).alias('moving_avg'))\n",
    " .orderBy('name')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the cumulative amount spent by each customer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL: Using RANGE BETWEEN with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------------+----------+\n",
      "| name|     date|amount_spent|moving_avg|\n",
      "+-----+---------+------------+----------+\n",
      "|Alice| 5/1/2016|          50|        50|\n",
      "|Alice| 5/2/2016|          45|        95|\n",
      "|Alice| 5/3/2016|          55|       150|\n",
      "|Alice| 5/4/2016|          47|       197|\n",
      "|Alice| 5/5/2016|          19|       216|\n",
      "|  Bob| 5/1/2016|          25|        25|\n",
      "|  Bob|5/12/2016|          32|        57|\n",
      "|  Bob|5/13/2016|          10|        67|\n",
      "|  Bob| 5/4/2016|          29|        96|\n",
      "|  Bob| 5/9/2016|          27|       123|\n",
      "+-----+---------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT name, date, amount_spent, \n",
    "          |    ROUND(SUM(amount_spent) OVER (partition by name ORDER BY date ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW), 2) as moving_avg\n",
    "          |FROM expense\n",
    "          |ORDER BY name\n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API: Using RANGE BETWEEN with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = (Window\n",
    "              .partitionBy(expense_df['name'])\n",
    "              .orderBy(expense_df['date'])\n",
    "              .rangeBetween(Window.unboundedPreceding, Window.currentRow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+------------+---------+\n",
      "| name|     date|amount_spent|cum_spent|\n",
      "+-----+---------+------------+---------+\n",
      "|Alice| 5/3/2016|          55|      150|\n",
      "|Alice| 5/1/2016|          50|       50|\n",
      "|Alice| 5/4/2016|          47|      197|\n",
      "|Alice| 5/2/2016|          45|       95|\n",
      "|Alice| 5/5/2016|          19|      216|\n",
      "|  Bob| 5/1/2016|          25|       25|\n",
      "|  Bob|5/13/2016|          10|       67|\n",
      "|  Bob|5/12/2016|          32|       57|\n",
      "|  Bob| 5/4/2016|          29|       96|\n",
      "|  Bob| 5/9/2016|          27|      123|\n",
      "+-----+---------+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(expense_df\n",
    " .select('name', 'date', 'amount_spent', F.round(F.sum('amount_spent').over(windowSpec), 2).alias('cum_spent'))\n",
    " .orderBy('name')\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tractor Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = [\n",
    "    (1, 'Jan-03', 141),\n",
    "    (2, 'Feb-03', 157),\n",
    "    (3, 'Mar-03', 185),\n",
    "    (4, 'Apr-03', 199),\n",
    "    (5, 'May-03', 203),\n",
    "    (6, 'Jun-03', 189),\n",
    "    (7, 'Jul-03', 207),\n",
    "    (8, 'Aug-03', 207),\n",
    "    (9, 'Sep-03', 171),\n",
    "    (10, 'Oct-03', 150),\n",
    "    (11, 'Nov-03', 138),\n",
    "    (12, 'Dec-03', 165),\n",
    "    (13, 'Jan-04', 145),\n",
    "    (14, 'Feb-04', 168),\n",
    "    (15, 'Mar-04', 197),\n",
    "    (16, 'Apr-04', 208),\n",
    "    (17, 'May-04', 210),\n",
    "    (18, 'Jun-04', 209),\n",
    "    (19, 'Jul-04', 238),\n",
    "    (20, 'Aug-04', 238)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_df = spark.createDataFrame(sales, ['tid', 'month_year', 'sales']).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tid: long (nullable = true)\n",
      " |-- month_year: string (nullable = true)\n",
      " |-- sales: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_df.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Difference of Current Sale from the Previous Sale:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQL: Using lag() with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+-----------+-----------+--------------------+\n",
      "|month_year|sales|sales_lag_1|sales_lag_2|sales_lag_3|diff_with_last_sales|\n",
      "+----------+-----+-----------+-----------+-----------+--------------------+\n",
      "|    Jan-03|  141|       null|       null|       null|                null|\n",
      "|    Feb-03|  157|        141|       null|       null|                  16|\n",
      "|    Mar-03|  185|        157|        141|       null|                  28|\n",
      "|    Apr-03|  199|        185|        157|        141|                  14|\n",
      "|    May-03|  203|        199|        185|        157|                   4|\n",
      "|    Jun-03|  189|        203|        199|        185|                 -14|\n",
      "|    Jul-03|  207|        189|        203|        199|                  18|\n",
      "|    Aug-03|  207|        207|        189|        203|                   0|\n",
      "|    Sep-03|  171|        207|        207|        189|                 -36|\n",
      "|    Oct-03|  150|        171|        207|        207|                 -21|\n",
      "|    Nov-03|  138|        150|        171|        207|                 -12|\n",
      "|    Dec-03|  165|        138|        150|        171|                  27|\n",
      "|    Jan-04|  145|        165|        138|        150|                 -20|\n",
      "|    Feb-04|  168|        145|        165|        138|                  23|\n",
      "|    Mar-04|  197|        168|        145|        165|                  29|\n",
      "|    Apr-04|  208|        197|        168|        145|                  11|\n",
      "|    May-04|  210|        208|        197|        168|                   2|\n",
      "|    Jun-04|  209|        210|        208|        197|                  -1|\n",
      "|    Jul-04|  238|        209|        210|        208|                  29|\n",
      "|    Aug-04|  238|        238|        209|        210|                   0|\n",
      "+----------+-----+-----------+-----------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(strip_margin(\n",
    "        \"\"\"SELECT month_year, sales, \n",
    "          |    LAG(sales, 1) OVER (ORDER BY tid) as sales_lag_1,\n",
    "          |    LAG(sales, 2) OVER (ORDER BY tid) as sales_lag_2,\n",
    "          |    LAG(sales, 3) OVER (ORDER BY tid) as sales_lag_3,\n",
    "          |    sales - LAG(sales, 1) OVER (ORDER BY tid) as diff_with_last_sales\n",
    "          |FROM sales \n",
    "        \"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DF API: Using RANGE BETWEEN with Window Function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "windowSpec = Window.orderBy(sales_df['tid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----------+-----------+-----------+--------------------+\n",
      "|month_year|sales|sales_lag_1|sales_lag_2|sales_lag_3|diff_with_last_sales|\n",
      "+----------+-----+-----------+-----------+-----------+--------------------+\n",
      "|    Jan-03|  141|       null|       null|       null|                null|\n",
      "|    Feb-03|  157|        141|       null|       null|                  16|\n",
      "|    Mar-03|  185|        157|        141|       null|                  28|\n",
      "|    Apr-03|  199|        185|        157|        141|                  14|\n",
      "|    May-03|  203|        199|        185|        157|                   4|\n",
      "|    Jun-03|  189|        203|        199|        185|                 -14|\n",
      "|    Jul-03|  207|        189|        203|        199|                  18|\n",
      "|    Aug-03|  207|        207|        189|        203|                   0|\n",
      "|    Sep-03|  171|        207|        207|        189|                 -36|\n",
      "|    Oct-03|  150|        171|        207|        207|                 -21|\n",
      "|    Nov-03|  138|        150|        171|        207|                 -12|\n",
      "|    Dec-03|  165|        138|        150|        171|                  27|\n",
      "|    Jan-04|  145|        165|        138|        150|                 -20|\n",
      "|    Feb-04|  168|        145|        165|        138|                  23|\n",
      "|    Mar-04|  197|        168|        145|        165|                  29|\n",
      "|    Apr-04|  208|        197|        168|        145|                  11|\n",
      "|    May-04|  210|        208|        197|        168|                   2|\n",
      "|    Jun-04|  209|        210|        208|        197|                  -1|\n",
      "|    Jul-04|  238|        209|        210|        208|                  29|\n",
      "|    Aug-04|  238|        238|        209|        210|                   0|\n",
      "+----------+-----+-----------+-----------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(sales_df\n",
    " .select('month_year', 'sales', \n",
    "         F.lag('sales', count=1).over(windowSpec).alias('sales_lag_1'),\n",
    "         F.lag('sales', count=2).over(windowSpec).alias('sales_lag_2'),\n",
    "         F.lag('sales', count=3).over(windowSpec).alias('sales_lag_3'),\n",
    "         (col('sales') - F.lag('sales', count=1).over(windowSpec)).alias('diff_with_last_sales'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "353px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
